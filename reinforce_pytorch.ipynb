{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in PyTorch\n",
    "\n",
    "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x11ecf8dc0>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, '_max_episode_steps'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits. \n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(state_dim[0], 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, n_actions)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
    "So, here gradient calculation is not needed.\n",
    "<br>\n",
    "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "to suppress gradient calculation.\n",
    "<br>\n",
    "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
    "<br>\n",
    "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
    "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
    "<br>\n",
    "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "\n",
    "    logits = model(torch.FloatTensor(states))\n",
    "    softmax = torch.softmax(logits, dim=-1).detach().numpy()\n",
    "\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "assert isinstance(test_probas, np.ndarray), \\\n",
    "    \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
    "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=10000):\n",
    "    \"\"\" \n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session \n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "    \n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    cum_rewards = np.zeros(len(rewards))\n",
    "    cum_rewards[-1] = rewards[-1]\n",
    "\n",
    "    for i in range(len(cum_rewards) - 2, -1, -1):\n",
    "        cum_rewards[i] = rewards[i] + cum_rewards[i + 1] * gamma\n",
    "\n",
    "    return cum_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-3):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(\n",
    "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
    "   \n",
    "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
    "    entropy = - torch.mean(torch.sum(probs * log_probs), dim=-1)\n",
    "    loss = - torch.mean(log_probs_for_actions * cumulative_returns) - entropy * entropy_coef\n",
    "\n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:108.250\n",
      "mean reward:123.890\n",
      "mean reward:157.860\n",
      "mean reward:220.740\n",
      "mean reward:264.370\n",
      "mean reward:214.970\n",
      "mean reward:183.160\n",
      "mean reward:178.470\n",
      "mean reward:210.600\n",
      "mean reward:439.970\n",
      "mean reward:524.840\n",
      "mean reward:192.840\n",
      "mean reward:9.630\n",
      "mean reward:39.350\n",
      "mean reward:66.210\n",
      "mean reward:9.820\n",
      "mean reward:87.940\n",
      "mean reward:228.740\n",
      "mean reward:150.250\n",
      "mean reward:15.930\n",
      "mean reward:9.690\n",
      "mean reward:9.320\n",
      "mean reward:9.490\n",
      "mean reward:9.830\n",
      "mean reward:135.130\n",
      "mean reward:419.170\n",
      "mean reward:1930.550\n",
      "mean reward:1336.750\n",
      "mean reward:1426.390\n",
      "mean reward:2168.280\n",
      "mean reward:1065.460\n",
      "mean reward:1698.050\n",
      "mean reward:684.860\n",
      "mean reward:455.130\n",
      "mean reward:681.070\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
    "    \n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "    \n",
    "    if np.mean(rewards) > 5000:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "def generate_session_for_present(env, t_max=10000):\n",
    "    \"\"\"\n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            print(t)\n",
    "            break\n",
    "    print(np.sum(rewards))\n",
    "    return states, actions, rewards\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n",
      "207.0\n",
      "227\n",
      "228.0\n",
      "208\n",
      "209.0\n",
      "213\n",
      "214.0\n",
      "219\n",
      "220.0\n",
      "212\n",
      "213.0\n",
      "211\n",
      "212.0\n",
      "228\n",
      "229.0\n",
      "217\n",
      "218.0\n",
      "242\n",
      "243.0\n",
      "225\n",
      "226.0\n",
      "205\n",
      "206.0\n",
      "211\n",
      "212.0\n",
      "232\n",
      "233.0\n",
      "235\n",
      "236.0\n",
      "207\n",
      "208.0\n",
      "220\n",
      "221.0\n",
      "222\n",
      "223.0\n",
      "234\n",
      "235.0\n",
      "202\n",
      "203.0\n",
      "200\n",
      "201.0\n",
      "227\n",
      "228.0\n",
      "209\n",
      "210.0\n",
      "212\n",
      "213.0\n",
      "230\n",
      "231.0\n",
      "253\n",
      "254.0\n",
      "219\n",
      "220.0\n",
      "223\n",
      "224.0\n",
      "215\n",
      "216.0\n",
      "218\n",
      "219.0\n",
      "235\n",
      "236.0\n",
      "197\n",
      "198.0\n",
      "243\n",
      "244.0\n",
      "220\n",
      "221.0\n",
      "211\n",
      "212.0\n",
      "233\n",
      "234.0\n",
      "223\n",
      "224.0\n",
      "234\n",
      "235.0\n",
      "220\n",
      "221.0\n",
      "220\n",
      "221.0\n",
      "237\n",
      "238.0\n",
      "217\n",
      "218.0\n",
      "212\n",
      "213.0\n",
      "216\n",
      "217.0\n",
      "217\n",
      "218.0\n",
      "225\n",
      "226.0\n",
      "229\n",
      "230.0\n",
      "214\n",
      "215.0\n",
      "215\n",
      "216.0\n",
      "221\n",
      "222.0\n",
      "220\n",
      "221.0\n",
      "229\n",
      "230.0\n",
      "215\n",
      "216.0\n",
      "237\n",
      "238.0\n",
      "214\n",
      "215.0\n",
      "207\n",
      "208.0\n",
      "236\n",
      "237.0\n",
      "229\n",
      "230.0\n",
      "212\n",
      "213.0\n",
      "207\n",
      "208.0\n",
      "213\n",
      "214.0\n",
      "206\n",
      "207.0\n",
      "213\n",
      "214.0\n",
      "232\n",
      "233.0\n",
      "236\n",
      "237.0\n",
      "224\n",
      "225.0\n",
      "211\n",
      "212.0\n",
      "242\n",
      "243.0\n",
      "216\n",
      "217.0\n",
      "227\n",
      "228.0\n",
      "238\n",
      "239.0\n",
      "219\n",
      "220.0\n",
      "248\n",
      "249.0\n",
      "217\n",
      "218.0\n",
      "201\n",
      "202.0\n",
      "231\n",
      "232.0\n",
      "202\n",
      "203.0\n",
      "226\n",
      "227.0\n",
      "226\n",
      "227.0\n",
      "228\n",
      "229.0\n",
      "224\n",
      "225.0\n",
      "214\n",
      "215.0\n",
      "216\n",
      "217.0\n",
      "204\n",
      "205.0\n",
      "209\n",
      "210.0\n",
      "216\n",
      "217.0\n",
      "219\n",
      "220.0\n",
      "235\n",
      "236.0\n",
      "236\n",
      "237.0\n",
      "249\n",
      "250.0\n",
      "224\n",
      "225.0\n",
      "233\n",
      "234.0\n",
      "217\n",
      "218.0\n",
      "224\n",
      "225.0\n",
      "224\n",
      "225.0\n",
      "213\n",
      "214.0\n",
      "224\n",
      "225.0\n",
      "219\n",
      "220.0\n",
      "223\n",
      "224.0\n",
      "207\n",
      "208.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    generate_session_for_present(env)\n",
    "None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session ended\n"
     ]
    }
   ],
   "source": [
    "# Record sessions\n",
    "\n",
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
    "    sessions = [generate_session_for_present(env_monitor) for _ in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n<video width=\"1280\" height=\"720\" controls>\n  <source src=\"videos/openaigym.video.3.13683.video000001.mp4\" type=\"video/mp4\">\n</video>\n"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"1280\" height=\"720\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-1]))  # You can also try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}